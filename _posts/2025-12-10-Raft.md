---
layout: post
title: "The Raft Consensus Algorithm"
subtitle: "Understanding the fundamental concepts behind the Raft consensus algorithm."
date: 2025-12-10
categories: distributed-systems Raft consensus
---

As companies scale to millions of users, the infrastructure they use to run their services hits a bottleneck. Compute, memory, storage, and network limitations all make it difficult to scale beyond a certain point. This leads companies to make use of distributed technologies. They distribute their load over multiple instances of the same service. An example of this would be using 3 different database instances for your application. However, when distributed systems are used, a new problem arises. Keeping those distributed systems consistent with each other is difficult, as problems such as network partition, hardware failures, etc., are very common. This is where consensus algorithms come into the picture. The objective of consensus algorithms is to ensure that distributed systems stay consistent with each other and fix any inconsistencies. Raft is an example of a consensus algorithm.

There are two ways to keep distributed systems in sync. State transfer is when a machine (usually a leader in the cluster) transfers its entire state to other machines regularly. The other machines will use this state to update their own state, thus keeping them consistent with each other. The first major issue that arises in this approach is the size of the state. If the state is large, for example with databases, logs, etc., then this can be a bottleneck. There is also a chance that the state may change as you are transferring it. Consider a leader which has a variable A in its state; let’s say it is initialized to 0. The leader takes a snapshot of it and sends it to other machines in the cluster. However, as the snapshot is being sent, the leader updates its variable to 1. All the other machines still have 0. This leads to inconsistency.

<div style="text-align: center;">
  <img src="/assets/images/Raft/Raft_StateTransfer.png" alt="Three scenarios" width="500">
  <p style="font-style: italic; font-size: 0.9em;">Figure 1: Leader gets a new update after sending state, leading to inconsistency.</p>
</div>

The second way is called replicated state machine. In this method, each command/request/change that the leader makes to its state is immediately sent to the other nodes in the cluster via a log. Raft is based on replicated state machines. The leader handles all requests that the client sends to the cluster. Before the request is processed, the leader will add it to the log. This log will be sent over to all the other nodes in the cluster. Only when a majority of the nodes acknowledge a log item is the change actually made to the machine. Although the round trip for acknowledgement leads to additional latency, it gives a better guarantee of consistency. Raft prioritizes consistency over horizontal scalability.

<div style="text-align: center;">
  <img src="/assets/images/Raft/Raft_ReplicatedStateMachine.png" alt="Three scenarios" width="500">
  <p style="font-style: italic; font-size: 0.9em;">Figure 2: Shows how a replicated state machine solves the inconsistency issue of state transfer.</p>
</div>

# Introduction
Raft was developed to address the complexities of an older consensus algorithm named Paxos. Raft uses a strong leader. This means that at any point in time, there is a single leader, and that leader alone handles the writes that the client requests. Each request goes through the leader, and it is the responsibility of the leader to send log entries to all the other nodes in the cluster and keep them consistent. The algorithm has three major components: Leader Election (which deals with how a leader is elected in case the previous one is unreachable), Log Replication (which deals with replicating the log entries to other nodes in the cluster), and Safety (which deals with ensuring that if a node receives a log entry for a particular index, then other nodes should never get a different command for that same log index).

Nodes in Raft can be in one of three states at any given moment — Follower, Candidate, or Leader. The leader, as we saw, handles all the requests and replicates log entries to other nodes. Followers are the other nodes that receive log entries from the leader and apply them to their own state. When a follower detects that a leader is unreachable or has not sent a request for a certain amount of time, it will turn into a Candidate. Candidates conduct an election, and if they win, they turn into a leader. Raft uses a logical clock to order messages. This logical time is called terms. Terms are numbered with a consecutive number, and a new term starts with a leader election. If a candidate or leader discovers that its term is out of date, then it will immediately revert to being a follower to avoid any inconsistencies.

# Leader Election
When a new cluster is created or when a node stops receiving communication from the leader, it initiates a leader election. To avoid all the nodes in a cluster starting an election at the same time (leading to split votes), a random timeout is added before the node starts an election. To start an election, the node will increment its current term and set its own state to “Candidate.” It will then send a `RequestVotes` RPC to all the other nodes in the cluster. To ensure consistency, a vote is only cast if the candidate’s log is at least as up to date as the receiver’s log. If the latest log has a different term, then the one with the higher term is the latest. If they have the same term, then the one with the longer log is the latest.

There are three scenarios for a candidate — Win, Lose, and Split. A candidate wins the election if it receives a majority of the votes. In this case, it becomes the Leader and starts sending empty `AppendEntries` RPCs as heartbeats (`AppendEntries` is described later). The heartbeats serve two purposes. They indicate the health of a leader — if a leader stops sending heartbeats, then another node can start an election. They also send information such as the commit index (explained in Log Replication). If a candidate discovers another leader with a higher term during the election, it will step down and revert to being a follower. If the candidate does not receive a majority of the votes, it will continue being a candidate until its election timer elapses; it will then retry. The last condition is when the votes are split. In such cases, another election is started after a random timeout. The randomness ensures that if two nodes are split in votes, then such a split does not keep occurring infinitely.

```
RequestVotes RPC:
Arguments
- term          candidates term
- candidateId   candidates ID
- lastLogIndex  index of candidate's last log entry
- lastLogTerm   term of candidate's last log entry

Results
- term          currentTerm for candidate to update itself
- voteGranted   true if the candidate receives a vote
```

# Log Replication
Log replication is the method by which the leader of the cluster keeps all the other nodes consistent. When the leader receives a request, it is first added to the leader's log. The leader then sends an `AppendEntries` request to all the other nodes in the cluster. The request replicates the client request on the other nodes. The receiver of an `AppendEntries` request will check the previous log index and previous log term (sent in the RPC) of the request. If they have a log entry at that index and term, then they will append the new entry to their log. If the follower does not have matching entries, it must delete conflicting entries after the mismatch and then append new ones. If a majority of the nodes successfully replicate the log entry on their log, the leader will commit the log entry. This means moving the commit index to the index of the log and also applying the command to the actual machine. This commit index is then sent via other `AppendEntries`. When a server receives an `AppendEntries` with an updated commit index, it will commit all entries up to that index. 

```
AppendEntries RPC:
Arguments
term  leaders term
leaderId  used to redirect clients to leader
prevLogIndex  index of log entry immediately preceeding new ones
prevLogTerm   term of latest log entry
entries[]     log entries to store
leaderCommit  leaders commit index

Results
term    term for the leader to update
success true if follower contained entry in matching previous index and term.
```

# Conclusion
Raft is used in several real world distributed systems `etcd`, `CockroachDB`, `Consul` all use Raft to achieve consistency. Raft made consensus algorithms simpler, and more intuitive. 